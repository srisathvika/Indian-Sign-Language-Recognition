{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/srisa/OneDrive/Desktop/NehaProject/ISL_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all libraries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "print(\"Loaded all libraries\")\n",
    "import os\n",
    "import cv2\n",
    "import glob as gb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 571 images belonging to 23 classes.\n",
      "Found 131 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "train_batch = train_gen.flow_from_directory(\n",
    "    directory=path,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=64,\n",
    "    subset='training',\n",
    "    class_mode = 'categorical',\n",
    "    seed=64\n",
    ")\n",
    "valid_batch = train_gen.flow_from_directory(\n",
    "    directory=path,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=64,\n",
    "    subset='validation',\n",
    "    class_mode = 'categorical',\n",
    "    seed=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in VGG19 : 22\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 9, 9, 512)         20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 23)                11799     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20036183 (76.43 MB)\n",
      "Trainable params: 11810839 (45.05 MB)\n",
      "Non-trainable params: 8225344 (31.38 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "9/9 [==============================] - 311s 34s/step - loss: 3.1875 - accuracy: 0.0438 - val_loss: 3.1207 - val_accuracy: 0.0687\n",
      "Epoch 2/25\n",
      "9/9 [==============================] - 289s 33s/step - loss: 3.1019 - accuracy: 0.0648 - val_loss: 3.1215 - val_accuracy: 0.0611\n",
      "Epoch 3/25\n",
      "9/9 [==============================] - 360s 41s/step - loss: 3.0761 - accuracy: 0.0718 - val_loss: 3.0713 - val_accuracy: 0.0763\n",
      "Epoch 4/25\n",
      "9/9 [==============================] - 363s 41s/step - loss: 2.9923 - accuracy: 0.1016 - val_loss: 3.0150 - val_accuracy: 0.0992\n",
      "Epoch 5/25\n",
      "9/9 [==============================] - 336s 38s/step - loss: 2.8897 - accuracy: 0.1594 - val_loss: 2.9229 - val_accuracy: 0.1527\n",
      "Epoch 6/25\n",
      "9/9 [==============================] - 340s 38s/step - loss: 2.6643 - accuracy: 0.2189 - val_loss: 3.0204 - val_accuracy: 0.1679\n",
      "Epoch 7/25\n",
      "9/9 [==============================] - 346s 39s/step - loss: 2.4787 - accuracy: 0.2662 - val_loss: 2.7056 - val_accuracy: 0.1298\n",
      "Epoch 8/25\n",
      "9/9 [==============================] - 322s 36s/step - loss: 2.0797 - accuracy: 0.3958 - val_loss: 2.5668 - val_accuracy: 0.3282\n",
      "Epoch 9/25\n",
      "9/9 [==============================] - 586s 69s/step - loss: 1.6558 - accuracy: 0.5044 - val_loss: 2.2313 - val_accuracy: 0.3893\n",
      "Epoch 10/25\n",
      "9/9 [==============================] - 358s 41s/step - loss: 1.4352 - accuracy: 0.5779 - val_loss: 2.3402 - val_accuracy: 0.3588\n",
      "Epoch 11/25\n",
      "9/9 [==============================] - 317s 36s/step - loss: 1.2965 - accuracy: 0.6357 - val_loss: 2.2150 - val_accuracy: 0.4504\n",
      "Epoch 12/25\n",
      "9/9 [==============================] - 314s 35s/step - loss: 1.1038 - accuracy: 0.7040 - val_loss: 2.0434 - val_accuracy: 0.4351\n",
      "Epoch 13/25\n",
      "9/9 [==============================] - 404s 47s/step - loss: 0.7979 - accuracy: 0.7741 - val_loss: 1.9766 - val_accuracy: 0.5115\n",
      "Epoch 14/25\n",
      "9/9 [==============================] - 354s 39s/step - loss: 0.6085 - accuracy: 0.8214 - val_loss: 1.7503 - val_accuracy: 0.5420\n",
      "Epoch 15/25\n",
      "9/9 [==============================] - 390s 44s/step - loss: 0.5219 - accuracy: 0.8546 - val_loss: 1.7743 - val_accuracy: 0.5496\n",
      "Epoch 16/25\n",
      "9/9 [==============================] - 390s 44s/step - loss: 0.4170 - accuracy: 0.8897 - val_loss: 1.3989 - val_accuracy: 0.5954\n",
      "Epoch 17/25\n",
      "9/9 [==============================] - 327s 37s/step - loss: 0.2688 - accuracy: 0.9089 - val_loss: 1.9946 - val_accuracy: 0.5115\n",
      "Epoch 18/25\n",
      "9/9 [==============================] - 1159s 141s/step - loss: 0.3054 - accuracy: 0.9194 - val_loss: 1.5827 - val_accuracy: 0.5725\n",
      "Epoch 19/25\n",
      "9/9 [==============================] - 325s 37s/step - loss: 0.2291 - accuracy: 0.9440 - val_loss: 1.9204 - val_accuracy: 0.5802\n",
      "Epoch 20/25\n",
      "9/9 [==============================] - 324s 36s/step - loss: 0.2244 - accuracy: 0.9335 - val_loss: 1.8540 - val_accuracy: 0.6031\n",
      "Epoch 21/25\n",
      "9/9 [==============================] - 337s 38s/step - loss: 0.2040 - accuracy: 0.9387 - val_loss: 1.6740 - val_accuracy: 0.5878\n",
      "Epoch 22/25\n",
      "9/9 [==============================] - 612s 34s/step - loss: 0.1996 - accuracy: 0.9457 - val_loss: 1.7011 - val_accuracy: 0.6107\n",
      "Epoch 23/25\n",
      "9/9 [==============================] - 373s 36s/step - loss: 0.1540 - accuracy: 0.9527 - val_loss: 1.9024 - val_accuracy: 0.6031\n",
      "Epoch 24/25\n",
      "9/9 [==============================] - 326s 37s/step - loss: 0.1073 - accuracy: 0.9737 - val_loss: 1.6456 - val_accuracy: 0.6565\n",
      "Epoch 25/25\n",
      "9/9 [==============================] - 511s 37s/step - loss: 0.0877 - accuracy: 0.9702 - val_loss: 1.7712 - val_accuracy: 0.6565\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "vgg_model = Sequential()\n",
    "\n",
    "vgg_base_model = VGG19(include_top = False, weights=\"imagenet\", input_shape = (300,300,3))\n",
    "print(f'Number of layers in VGG19 : {len(vgg_base_model.layers)}')\n",
    "\n",
    "vgg_base_model.trainable = True\n",
    "for layer in vgg_base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_model.add(vgg_base_model)\n",
    "vgg_model.add(GlobalAveragePooling2D())\n",
    "vgg_model.add(Dense(units = 23, activation = 'softmax'))\n",
    "vgg_model.summary()\n",
    "\n",
    "\n",
    "vgg_model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.0001) , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "vgg = vgg_model.fit(train_batch, batch_size = 8, epochs = 25, validation_data = valid_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 702 files belonging to 23 classes.\n",
      "Using 562 files for training.\n",
      "Found 702 files belonging to 23 classes.\n",
      "Using 140 files for validation.\n"
     ]
    }
   ],
   "source": [
    "training_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    path,\n",
    "    batch_size = 8,\n",
    "    image_size =(300,300),\n",
    "\n",
    "    shuffle = True,\n",
    "    seed =123,\n",
    "    subset ='training',\n",
    "    validation_split=0.2\n",
    "    )\n",
    "validation_data =keras.preprocessing.image_dataset_from_directory(\n",
    "    path,\n",
    "    batch_size = 8,\n",
    "    image_size =(300,300),\n",
    "\n",
    "    shuffle = True,\n",
    "    seed =123,\n",
    "    validation_split =0.2,\n",
    "    subset ='validation'\n",
    "    \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 9s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet_model = Sequential()\n",
    "\n",
    "pretrained_model= keras.applications.ResNet50(include_top=False,\n",
    "                   input_shape=(300,300,3),\n",
    "                   pooling='avg',classes=20,\n",
    "                   weights='imagenet')\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(Flatten())\n",
    "resnet_model.add(Dense(1024, activation='relu'))\n",
    "resnet_model.add(Dense(23, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "resnet_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(lr=0.0001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "71/71 [==============================] - 305s 4s/step - loss: 3.2306 - accuracy: 0.0996 - val_loss: 3.2266 - val_accuracy: 0.0500\n",
      "Epoch 2/25\n",
      "71/71 [==============================] - 88s 1s/step - loss: 2.3693 - accuracy: 0.3203 - val_loss: 3.1859 - val_accuracy: 0.1071\n",
      "Epoch 3/25\n",
      "71/71 [==============================] - 89s 1s/step - loss: 1.8196 - accuracy: 0.5249 - val_loss: 2.9147 - val_accuracy: 0.1357\n",
      "Epoch 4/25\n",
      "71/71 [==============================] - 942s 13s/step - loss: 1.4239 - accuracy: 0.6655 - val_loss: 2.5968 - val_accuracy: 0.2714\n",
      "Epoch 5/25\n",
      "71/71 [==============================] - 83s 1s/step - loss: 1.1049 - accuracy: 0.7811 - val_loss: 2.7534 - val_accuracy: 0.2071\n",
      "Epoch 6/25\n",
      "71/71 [==============================] - 84s 1s/step - loss: 0.8539 - accuracy: 0.8612 - val_loss: 2.9103 - val_accuracy: 0.2071\n",
      "Epoch 7/25\n",
      "71/71 [==============================] - 85s 1s/step - loss: 0.6678 - accuracy: 0.9110 - val_loss: 2.3166 - val_accuracy: 0.3286\n",
      "Epoch 8/25\n",
      "71/71 [==============================] - 84s 1s/step - loss: 0.5243 - accuracy: 0.9484 - val_loss: 3.0483 - val_accuracy: 0.2000\n",
      "Epoch 9/25\n",
      "71/71 [==============================] - 82s 1s/step - loss: 0.4352 - accuracy: 0.9573 - val_loss: 2.4264 - val_accuracy: 0.2643\n",
      "Epoch 10/25\n",
      "71/71 [==============================] - 88s 1s/step - loss: 0.3549 - accuracy: 0.9733 - val_loss: 2.1246 - val_accuracy: 0.3929\n",
      "Epoch 11/25\n",
      "71/71 [==============================] - 90s 1s/step - loss: 0.2820 - accuracy: 0.9822 - val_loss: 2.0775 - val_accuracy: 0.3929\n",
      "Epoch 12/25\n",
      "71/71 [==============================] - 88s 1s/step - loss: 0.2418 - accuracy: 0.9911 - val_loss: 2.1552 - val_accuracy: 0.3643\n",
      "Epoch 13/25\n",
      "71/71 [==============================] - 89s 1s/step - loss: 0.2230 - accuracy: 0.9822 - val_loss: 2.1300 - val_accuracy: 0.3786\n",
      "Epoch 14/25\n",
      "71/71 [==============================] - 89s 1s/step - loss: 0.1883 - accuracy: 0.9822 - val_loss: 2.2650 - val_accuracy: 0.3214\n",
      "Epoch 15/25\n",
      "71/71 [==============================] - 138s 2s/step - loss: 0.1669 - accuracy: 0.9858 - val_loss: 2.7424 - val_accuracy: 0.2857\n",
      "Epoch 16/25\n",
      "71/71 [==============================] - 89s 1s/step - loss: 0.1630 - accuracy: 0.9893 - val_loss: 2.0359 - val_accuracy: 0.4000\n",
      "Epoch 17/25\n",
      "71/71 [==============================] - 93s 1s/step - loss: 0.1324 - accuracy: 0.9893 - val_loss: 2.0533 - val_accuracy: 0.3857\n",
      "Epoch 18/25\n",
      "71/71 [==============================] - 91s 1s/step - loss: 0.1274 - accuracy: 0.9840 - val_loss: 2.0306 - val_accuracy: 0.4143\n",
      "Epoch 19/25\n",
      "71/71 [==============================] - 100s 1s/step - loss: 0.1143 - accuracy: 0.9858 - val_loss: 2.0329 - val_accuracy: 0.4071\n",
      "Epoch 20/25\n",
      "71/71 [==============================] - 91s 1s/step - loss: 0.1108 - accuracy: 0.9840 - val_loss: 2.0465 - val_accuracy: 0.4071\n",
      "Epoch 21/25\n",
      "71/71 [==============================] - 88s 1s/step - loss: 0.1035 - accuracy: 0.9822 - val_loss: 2.0251 - val_accuracy: 0.4143\n",
      "Epoch 22/25\n",
      "71/71 [==============================] - 83s 1s/step - loss: 0.0944 - accuracy: 0.9893 - val_loss: 2.0782 - val_accuracy: 0.3929\n",
      "Epoch 23/25\n",
      "71/71 [==============================] - 83s 1s/step - loss: 0.0909 - accuracy: 0.9875 - val_loss: 2.0183 - val_accuracy: 0.4143\n",
      "Epoch 24/25\n",
      "71/71 [==============================] - 84s 1s/step - loss: 0.0913 - accuracy: 0.9875 - val_loss: 2.0316 - val_accuracy: 0.4214\n",
      "Epoch 25/25\n",
      "71/71 [==============================] - 637s 9s/step - loss: 0.0804 - accuracy: 0.9858 - val_loss: 2.0244 - val_accuracy: 0.4071\n"
     ]
    }
   ],
   "source": [
    "epochs=25\n",
    "history = resnet_model.fit(\n",
    "    \n",
    "  training_data,\n",
    "  validation_data=validation_data,\n",
    "  epochs=epochs\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
